# DuckDB version: v0.9.1
#### git hash: 401c8061c6ece35949cac58c7770cc755710ca86

# Disclaimer
All writing is my own. I've collaborated & shared ideas with Johan Laursen on the written parts, and discussed high level coding ideas, but all coding is my own with the aid of Copilot & stackoverflow where cited.

# 1.  Out of Tree Extension (20%) 
 
#### Create an out of tree extension, named oml, using the DuckDB extension template: https://github.com/duckdb/extension-template 
#### Question 1. Describe in one paragraph the build process when you compile the extension.
DuckDB uses "make" as a build entrypoint. Upon calling "make", the "release" action is called by default in "Makefile:58". This calls "CMake" twice: Once for the sub-repository "duckdb" which compiles the core of duckdb, and secondarily on the current directory (our extension). Both look for the "CMakeLists.txt" file, which describes high-level build goals, including other "CMakeLists.txt"s in other subdirectories [duckdb/CMakeLists.txt:1067~1079]. These high-level goals are then used to build lower level build goals in e.g. make or ninja. These tools also avoid recompiling code which has not changed, which means that any change to our extension only needs to recompile the modified files, and not the entire duckdb extension. The resulting binaries are put into ./build/release

# 2. Database Load (60%) 
#### Question 2 (20%). Describe how the single threaded version of the “read_csv” table function is  defined  in  DuckDB  (see  src/include/duckdb/function/table/read_csv.hpp and src/function/table/read_csv.cpp).
There's three core components to the read_csv table function. The bind-, init- and the table function. The bind function establishes metadata about the file read, and the inputs provided by the user. This reader supports either receiving the dtypes and column names as input parameters, or by inferring them using a "Sniffer" [duckdb/src/function/table/read_csv.cpp:110~111], which assigns a "most probably type" to each of the columns found in the csv. Finally, the csv may also detect a Hive partitioning folder structure [duckdb/src/function/table/read_csv.cpp:98] which can skip reading entire files if a selection predicate excludes them. 

The init function constructs a file reader from the user input.

Finally, the core table function parses the csv at a character-level in [duckdb/src/function/table/read_csv.cpp:798 & duckdb/src/execution/operator/csv_scanner/buffered_csv_reader.cpp:194~432], which keeps reading until a full vector is constructed, after which it is flushed [duckdb/src/execution/operator/csv_scanner/base_csv_reader.cpp:270~273]. This table function is called multiple times, producing multiple DataChunks, which are passed on to the next step in the pipeline (e.g. print to terminal).

SELECT MODE(subject), MODE(key), MODE(value), SUM(timestamp_s), SUM(timestamp_us), SUM(channel), SUM(rssi) FROM POWER_CONSUMPTION;

# Question  3  (40%) & 3. Database Generation (20%)
#### Design,  implement  and  test  a  table  function Power_Consumption_load(filename) that reads from the oml file ‘filename’ generated by an IoTLab measurement point and loads the tuples it contains into a table that corresponds to the Power_Consumption table above. (hint: iot_load is a simplified version of the single threaded csv reader). 
 
#### Question 5. Design,  implement and test a table function OmlGen(filename) that reads an oml file, create a schema based on the metadata the oml file contains and loads the tuples that the oml file contains.